name: Performance Regression Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # æ¯å¤© UTC 2:00 è¿è¡Œä¸€æ¬¡
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  performance-benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Install Rust toolchain
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          profile: minimal
          override: true
          components: rustfmt, clippy
      
      - name: Cache cargo registry
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-
      
      - name: Cache criterion results
        uses: actions/cache@v3
        with:
          path: target/criterion
          key: ${{ runner.os }}-criterion-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-criterion-
      
      - name: Build benchmarks
        run: cargo build --release --benches
      
      - name: Run all benchmarks
        run: |
          cargo bench --bench math_benchmarks || true
          cargo bench --bench ecs_benchmarks || true
          cargo bench --bench physics_benchmarks || true
          cargo bench --bench render_benchmarks || true
      
      - name: Download baseline data
        uses: actions/download-artifact@v3
        with:
          name: performance-baseline
          path: baseline/
        continue-on-error: true
      
      - name: Compare with baseline
        run: |
          # ç®€å•çš„æ€§èƒ½æ¯”è¾ƒè„šæœ¬
          cat > compare_baseline.sh << 'SCRIPT'
          #!/bin/bash
          set -e
          
          BASELINE_DIR="$1"
          CURRENT_DIR="$2"
          THRESHOLD_WARNING="${3:-10.0}"
          THRESHOLD_CRITICAL="${4:-20.0}"
          
          if [ ! -d "$BASELINE_DIR" ]; then
            echo "No baseline directory found, skipping comparison"
            exit 0
          fi
          
          CRITICAL_REGRESSION=0
          WARNING_REGRESSION=0
          
          echo "Comparing performance benchmarks..."
          echo "Warning threshold: ${THRESHOLD_WARNING}%"
          echo "Critical threshold: ${THRESHOLD_CRITICAL}%"
          echo ""
          
          # æŸ¥æ‰¾æ‰€æœ‰åŸºå‡†æµ‹è¯•ç»“æžœ
          find "$CURRENT_DIR/target/criterion" -name "estimates.json" | while read current_file; do
            # æå–åŸºå‡†æµ‹è¯•åç§°
            bench_path=$(dirname "$current_file")
            bench_name=$(basename "$(dirname "$bench_path")")
            
            baseline_file="$BASELINE_DIR/target/criterion/$bench_name/base/estimates.json"
            
            if [ ! -f "$baseline_file" ]; then
              echo "âš ï¸  No baseline for $bench_name (new benchmark?)"
              continue
            fi
            
            # æå–æ—¶é—´å€¼ï¼ˆä½¿ç”¨ jq æˆ–ç®€å•çš„ grepï¼‰
            baseline_time=$(grep -o '"point_estimate":[0-9.]*' "$baseline_file" | cut -d: -f2 || echo "0")
            current_time=$(grep -o '"point_estimate":[0-9.]*' "$current_file" | cut -d: -f2 || echo "0")
            
            if [ "$baseline_time" = "0" ] || [ "$current_time" = "0" ]; then
              continue
            fi
            
            # è®¡ç®—å›žå½’ç™¾åˆ†æ¯”
            regression=$(echo "scale=2; (($current_time - $baseline_time) / $baseline_time) * 100" | bc)
            
            if (( $(echo "$regression > $THRESHOLD_CRITICAL" | bc -l) )); then
              echo "ðŸ”´ CRITICAL: $bench_name - ${regression}% slower (baseline: ${baseline_time}ns, current: ${current_time}ns)"
              CRITICAL_REGRESSION=1
            elif (( $(echo "$regression > $THRESHOLD_WARNING" | bc -l) )); then
              echo "ðŸŸ¡ WARNING: $bench_name - ${regression}% slower (baseline: ${baseline_time}ns, current: ${current_time}ns)"
              WARNING_REGRESSION=1
            elif (( $(echo "$regression < -5.0" | bc -l) )); then
              echo "ðŸŸ¢ IMPROVED: $bench_name - ${regression#-}% faster (baseline: ${baseline_time}ns, current: ${current_time}ns)"
            else
              echo "âœ… OK: $bench_name - ${regression}% change (baseline: ${baseline_time}ns, current: ${current_time}ns)"
            fi
          done
          
          if [ $CRITICAL_REGRESSION -eq 1 ]; then
            echo ""
            echo "âŒ Critical performance regression detected!"
            exit 1
          elif [ $WARNING_REGRESSION -eq 1 ]; then
            echo ""
            echo "âš ï¸  Performance regression detected (warning only)"
            exit 0
          else
            echo ""
            echo "âœ… All benchmarks are within acceptable performance range"
            exit 0
          fi
          SCRIPT
          
          chmod +x compare_baseline.sh
          
          # å®‰è£… bcï¼ˆç”¨äºŽæµ®ç‚¹æ•°è®¡ç®—ï¼‰
          sudo apt-get update && sudo apt-get install -y bc || true
          
          # è¿è¡Œæ¯”è¾ƒ
          if [ -d "baseline" ] && [ "$(ls -A baseline 2>/dev/null)" ]; then
            ./compare_baseline.sh baseline . 10.0 20.0 || {
              echo "Performance regression detected!"
              exit 1
            }
          else
            echo "No baseline found, skipping comparison"
            echo "This is normal for the first run or when baseline is not available"
          fi
      
      - name: Generate performance summary
        run: |
          cat > performance_summary.md << 'EOF'
          # Performance Benchmark Results
          
          ## Summary
          
          This workflow ran performance benchmarks for the game engine.
          
          ### Benchmarks Run:
          - Math operations (`math_benchmarks`)
          - ECS operations (`ecs_benchmarks`)
          - Physics operations (`physics_benchmarks`)
          - Render operations (`render_benchmarks`)
          
          ### Results
          
          Detailed HTML reports are available in the workflow artifacts.
          
          Criterion results are stored in `target/criterion/` directory.
          
          EOF
          cat performance_summary.md
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results-${{ github.run_number }}
          path: |
            target/criterion/
            performance_summary.md
          retention-days: 30
      
      - name: Update baseline (on main branch)
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        uses: actions/upload-artifact@v3
        with:
          name: performance-baseline
          path: target/criterion/
          retention-days: 90
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            let comment = '## ðŸ“Š Performance Benchmark Results\n\n';
            comment += 'Performance benchmarks have been run for this PR.\n\n';
            comment += '### Benchmarks Executed:\n';
            comment += '- âœ… Math operations\n';
            comment += '- âœ… ECS operations\n';
            comment += '- âœ… Physics operations\n';
            comment += '- âœ… Render operations\n\n';
            comment += 'ðŸ“ **Detailed Results**: Check the workflow artifacts for HTML reports.\n\n';
            comment += 'ðŸ’¡ **Note**: If this is the first run, baseline comparison will be skipped.';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
